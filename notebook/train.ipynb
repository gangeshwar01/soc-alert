{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03a2cbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob as gb\n",
    "\n",
    "#Load dataset\n",
    "data=r\"D:\\RealTime_Alert_Analysis\\dataset\"\n",
    "\n",
    "#club all files into one\n",
    "files=gb.glob(f\"{data}/*.pcap_ISCX.csv\")\n",
    "df_list = [pd.read_csv(f) for f in files]\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# print(df.columns.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da2d0874",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df[' Label'] = le.fit_transform(df[' Label'])\n",
    "\n",
    "# print(\"Successfully! Labels encoded.\")\n",
    "# print(dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80f0e102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataset into features and labels\n",
    "\n",
    "X = df.drop(' Label', axis=1)\n",
    "y = df[' Label']\n",
    "\n",
    "# print(X,y)\n",
    "# print(X.shape, y.shape)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Replace inf and -inf with NaN\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "X = X.dropna()\n",
    "y = y.loc[X.index]\n",
    "\n",
    "#Splitting dataset into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)\n",
    "\n",
    "# print(\"Training samples:\", X_train.shape[0])\n",
    "# print(\"Testing samples:\", x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b27d3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Random Forest model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=15, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"‚úÖ Random Forest model trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2209fce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9982124418292149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RealTime_Alert_Analysis\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    454265\n",
      "           1       0.96      0.37      0.53       391\n",
      "           2       1.00      1.00      1.00     25605\n",
      "           3       1.00      0.97      0.99      2059\n",
      "           4       1.00      1.00      1.00     46025\n",
      "           5       0.99      0.98      0.99      1100\n",
      "           6       1.00      0.99      1.00      1159\n",
      "           7       1.00      1.00      1.00      1587\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       1.00      0.29      0.44         7\n",
      "          10       0.99      1.00      1.00     31761\n",
      "          11       1.00      0.99      1.00      1180\n",
      "          12       0.66      0.81      0.73       301\n",
      "          13       0.00      0.00      0.00         4\n",
      "          14       1.00      0.01      0.02       130\n",
      "\n",
      "    accuracy                           1.00    565576\n",
      "   macro avg       0.91      0.76      0.78    565576\n",
      "weighted avg       1.00      1.00      1.00    565576\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RealTime_Alert_Analysis\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\RealTime_Alert_Analysis\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5617c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Scanning dataset for non-numeric / IP-like columns...\n",
      "\n",
      "‚ö† Found 29 problematic columns:\n",
      "\n",
      "üß± Column: src_ip\n",
      "   ‚Üí Type: object\n",
      "   ‚Üí IP-like values: 987993\n",
      "   ‚Üí Example entries: ['3.122.49.24', '192.168.1.79', '192.168.1.152', '192.168.1.193', '192.168.1.250']\n",
      "\n",
      "üß± Column: dst_ip\n",
      "   ‚Üí Type: object\n",
      "   ‚Üí IP-like values: 987993\n",
      "   ‚Üí Example entries: ['192.168.1.152', '192.168.1.255', '192.168.1.190', '172.217.167.67', '216.58.199.66']\n",
      "\n",
      "üß± Column: proto\n",
      "   ‚Üí Type: object\n",
      "   ‚Üí IP-like values: 0\n",
      "   ‚Üí Example entries: ['tcp', 'udp', 'icmp']\n",
      "\n",
      "üß± Column: service\n",
      "   ‚Üí Type: object\n",
      "   ‚Üí IP-like values: 0\n",
      "   ‚Üí Example entries: ['-', 'dns', 'ssl', 'dhcp', 'http']\n",
      "\n",
      "üß± Column: src_bytes\n",
      "   ‚Üí Type: object\n",
      "   ‚Üí IP-like values: 175\n",
      "   ‚Üí Example entries: ['0.0.0.0']\n",
      "\n",
      "üß± Column: conn_state\n",
      "   ‚Üí Type: object\n",
      "   ‚Üí IP-like values: 0\n",
      "   ‚Üí Example entries: ['OTH', 'S0', 'SHR', 'RSTRH', 'SH']\n",
      "\n",
      "üß± Column: dns_query\n",
      "   ‚Üí Type: object\n",
      "   ‚Üí IP-like values: 0\n",
      "   ‚Üí Example entries: ['-', '_sleep-proxy._udp.local', '_raop._tcp.local', '_ipps._tcp.local', 'ISATAP']\n",
      "\n",
      "üß± Column: dns_AA\n",
      "   ‚Üí Type: object\n",
      "   ‚Üí IP-like values: 0\n",
      "   ‚Üí Example entries: ['-', 'F', 'T']\n",
      "\n",
      "üß± Column: dns_RD\n",
      "   ‚Üí Type: object\n",
      "   ‚Üí IP-like values: 0\n",
      "   ‚Üí Example entries: ['-', 'F', 'T']\n",
      "\n",
      "üß± Column: dns_RA\n",
      "   ‚Üí Type: object\n",
      "   ‚Üí IP-like values: 0\n",
      "   ‚Üí Example entries: ['-', 'F', 'T']\n",
      "\n",
      "üß± Column: dns_rejected\n",
      "   ‚Üí Type: object\n",
      "   ‚Üí IP-like values: 0\n",
      "   ‚Üí Example entries: ['-', 'F', 'T']\n",
      "\n",
      "üß± Column: ssl_version\n",
      "   ‚Üí Type: object\n",
      "   ‚Üí IP-like values: 0\n",
      "   ‚Üí Example entries: ['-', 'TLSv12', 'TLSv13']\n",
      "\n",
      "üß± Column: ssl_cipher\n",
      "   ‚Üí Type: object\n",
      "   ‚Üí IP-like values: 0\n",
      "   ‚Üí Example entries: ['-', 'TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256', 'TLS_AES_128_GCM_SHA256', 'TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256', 'TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384']\n",
      "\n",
      "üß± Column: ssl_resumed\n",
      "   ‚Üí Type: object\n",
      "   ‚Üí IP-like values: 0\n",
      "   ‚Üí Example entries: ['-', 'F', 'T']\n",
      "\n",
      "üß± Column: ssl_established\n",
      "   ‚Üí Type: object\n",
      "   ‚Üí IP-like values: 0\n",
      "   ‚Üí Example entries: ['-', 'F', 'T']\n",
      "\n",
      "üß± Column: ssl_subject\n",
      "   ‚Üí Type: object\n",
      "   ‚Üí IP-like values: 0\n",
      "   ‚Üí Example entries: ['-', 'CN=settings-win.data.microsoft.com;OU=WSE;O=Microsoft;L=Redmond;ST=WA;C=US', 'CN=*.wns.windows.com', 'CN=sls.update.microsoft.com;OU=DSP;O=Microsoft;L=Redmond;ST=WA;C=US', 'CN=*.events.data.microsoft.com;OU=Microsoft;O=Microsoft Corporation;L=Redmond;ST=WA;C=US']\n",
      "\n",
      "üß± Column: ssl_issuer\n",
      "   ‚Üí Type: object\n",
      "   ‚Üí IP-like values: 0\n",
      "   ‚Üí Example entries: ['-', 'CN=Microsoft Secure Server CA 2011;O=Microsoft Corporation;L=Redmond;ST=Washington;C=US', 'CN=Microsoft IT TLS CA 5;OU=Microsoft IT;O=Microsoft Corporation;L=Redmond;ST=Washington;C=US', 'CN=Microsoft Update Secure Server CA 2.1;O=Microsoft Corporation;L=Redmond;ST=Washington;C=US']\n",
      "\n",
      "üß± Column: http_trans_depth\n",
      "   ‚Üí Type: object\n",
      "   ‚Üí IP-like values: 0\n",
      "   ‚Üí Example entries: ['-']\n",
      "\n",
      "üß± Column: http_method\n",
      "   ‚Üí Type: object\n",
      "   ‚Üí IP-like values: 0\n",
      "   ‚Üí Example entries: ['-', 'GET', 'POST', 'HEAD']\n",
      "\n",
      "üß± Column: http_uri\n",
      "   ‚Üí Type: object\n",
      "   ‚Üí IP-like values: 0\n",
      "   ‚Üí Example entries: ['-', '/no_nonce_string/ContentDirectory/scpd.xml', '/no_nonce_string/', '/emdl/d/dod/ph/prod6/msdownload/update/software/defu/2019/02/1024/updateplatform_23725566d66a26c81fbfaf79701b727f92a650ea.exe.json', '/d/msdownload/update/software/defu/2019/02/updateplatform_23725566d66a26c81fbfaf79701b727f92a650ea.exe']\n",
      "\n",
      "üß± Column: http_referrer\n",
      "   ‚Üí Type: object\n",
      "   ‚Üí IP-like values: 0\n",
      "   ‚Üí Example entries: ['-']\n",
      "\n",
      "üß± Column: http_version\n",
      "   ‚Üí Type: object\n",
      "   ‚Üí IP-like values: 0\n",
      "   ‚Üí Example entries: ['-']\n",
      "\n",
      "üß± Column: http_user_agent\n",
      "   ‚Üí Type: object\n",
      "   ‚Üí IP-like values: 0\n",
      "   ‚Üí Example entries: ['-', 'Microsoft-Windows/10.0 UPnP/1.0', 'DAFUPnP', 'Microsoft-Delivery-Optimization/10.0', 'MICROSOFT_DEVICE_METADATA_RETRIEVAL_CLIENT']\n",
      "\n",
      "üß± Column: http_orig_mime_types\n",
      "   ‚Üí Type: object\n",
      "   ‚Üí IP-like values: 0\n",
      "   ‚Üí Example entries: ['-', 'application/xml', 'application/soap+xml']\n",
      "\n",
      "üß± Column: http_resp_mime_types\n",
      "   ‚Üí Type: object\n",
      "   ‚Üí IP-like values: 0\n",
      "   ‚Üí Example entries: ['-', 'application/ocsp-response', 'application/xml', 'text/json', 'text/plain']\n",
      "\n",
      "üß± Column: weird_name\n",
      "   ‚Üí Type: object\n",
      "   ‚Üí IP-like values: 0\n",
      "   ‚Üí Example entries: ['bad_TCP_checksum', '-', 'bad_UDP_checksum', 'active_connection_reuse', 'data_before_established']\n",
      "\n",
      "üß± Column: weird_addl\n",
      "   ‚Üí Type: object\n",
      "   ‚Üí IP-like values: 0\n",
      "   ‚Üí Example entries: ['-']\n",
      "\n",
      "üß± Column: weird_notice\n",
      "   ‚Üí Type: object\n",
      "   ‚Üí IP-like values: 0\n",
      "   ‚Üí Example entries: ['F', '-']\n",
      "\n",
      "üß± Column: type\n",
      "   ‚Üí Type: object\n",
      "   ‚Üí IP-like values: 0\n",
      "   ‚Üí Example entries: ['normal', 'scanning']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(r\"D:\\RealTime_Alert_Analysis\\dataset\\Network_dataset_1.csv\", low_memory=False)\n",
    "\n",
    "ip_regex = re.compile(r\"^\\d{1,3}(\\.\\d{1,3}){3}$\")\n",
    "\n",
    "print(\"\\nüîç Scanning dataset for non-numeric / IP-like columns...\\n\")\n",
    "\n",
    "problem_cols = []\n",
    "for col in df.columns:\n",
    "    # Check column type and contents\n",
    "    s = df[col].astype(str)\n",
    "    ip_like_count = s.str.match(ip_regex).sum()\n",
    "    try:\n",
    "        pd.to_numeric(s, errors=\"raise\")\n",
    "    except Exception:\n",
    "        # Record only if at least 1 problematic entry\n",
    "        bad_examples = s[s.str.match(ip_regex) | ~s.str.match(r\"^-?\\d+(\\.\\d+)?$\")].unique()[:5]\n",
    "        problem_cols.append({\n",
    "            \"Column\": col,\n",
    "            \"Type\": df[col].dtype,\n",
    "            \"IP_like_values\": int(ip_like_count),\n",
    "            \"Example_Entries\": list(bad_examples)\n",
    "        })\n",
    "\n",
    "if not problem_cols:\n",
    "    print(\"‚úÖ All columns are clean and numeric!\")\n",
    "else:\n",
    "    print(f\"‚ö† Found {len(problem_cols)} problematic columns:\\n\")\n",
    "    for item in problem_cols:\n",
    "        print(f\"üß± Column: {item['Column']}\")\n",
    "        print(f\"   ‚Üí Type: {item['Type']}\")\n",
    "        print(f\"   ‚Üí IP-like values: {item['IP_like_values']}\")\n",
    "        print(f\"   ‚Üí Example entries: {item['Example_Entries']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aadd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Columns still containing IP-like strings (even if numeric):\n",
      "src_ip       987993\n",
      "dst_ip       987993\n",
      "src_bytes       175\n",
      "dtype: int64\n",
      "\n",
      "üß± src_ip: ['3.122.49.24' '192.168.1.79' '192.168.1.152' '192.168.1.193'\n",
      " '192.168.1.250' '192.168.1.133' '192.168.1.192' '192.168.1.103'\n",
      " '192.168.1.46' '192.168.1.195']\n",
      "\n",
      "üß± dst_ip: ['192.168.1.152' '192.168.1.255' '192.168.1.190' '172.217.167.67'\n",
      " '216.58.199.66' '224.0.0.251' '52.40.98.101' '52.35.215.194'\n",
      " '117.18.237.29' '192.168.1.192']\n",
      "\n",
      "üß± src_bytes: ['0.0.0.0']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"D:\\RealTime_Alert_Analysis\\dataset\\Network_dataset_1.csv\", low_memory=False)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# üîç  Deep clean just before inference  (catch any residual IP strings)\n",
    "# ------------------------------------------------------------------\n",
    "ip_pattern = re.compile(r\"^\\d{1,3}(\\.\\d{1,3}){3}$\")\n",
    "\n",
    "# find any column that still contains IP-like strings\n",
    "bad_cols = []\n",
    "for c in X.columns:\n",
    "    if X[c].astype(str).str.match(ip_pattern).any():\n",
    "        bad_cols.append(c)\n",
    "        # replace all IP-like entries with 0\n",
    "        X[c] = X[c].astype(str).apply(lambda v: 0 if ip_pattern.match(v) else v)\n",
    "\n",
    "# force numeric conversion one more time\n",
    "X = X.apply(pd.to_numeric, errors=\"coerce\").fillna(0).astype(np.float32)\n",
    "\n",
    "if bad_cols:\n",
    "    st.warning(f\"üßπ Sanitized residual IP-like values in: {', '.join(bad_cols)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af1fe901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Realistic test dataset created ‚Üí D:\\RealTime_Alert_Analysis\\dataset\\realistic_balanced_attacks.csv\n",
      "Features: 78 | Classes: 10 | Rows: 2000\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "MODEL_PATH = r\"D:\\RealTime_Alert_Analysis\\report\\training_report\\trained_model_balanced_final_fixed.joblib\"\n",
    "OUTPUT_PATH = r\"D:\\RealTime_Alert_Analysis\\dataset\\realistic_balanced_attacks.csv\"\n",
    "\n",
    "# Load model metadata\n",
    "bundle = joblib.load(MODEL_PATH)\n",
    "features = bundle.get(\"feature_names\", [])\n",
    "classes = [\n",
    "    \"BENIGN\", \"DDoS\", \"PortScan\", \"Botnet\",\n",
    "    \"Infiltration\", \"Web Attack ‚Äì Brute Force\",\n",
    "    \"Web Attack ‚Äì XSS\", \"Web Attack ‚Äì Sql Injection\",\n",
    "    \"Heartbleed\", \"ANOMALY\"\n",
    "]\n",
    "\n",
    "np.random.seed(42)\n",
    "data = []\n",
    "\n",
    "# Generate synthetic data with realistic patterns\n",
    "for label in classes:\n",
    "    for _ in range(200):\n",
    "        if label == \"BENIGN\":\n",
    "            row = np.random.normal(0.3, 0.05, len(features))\n",
    "        elif label == \"DDoS\":\n",
    "            row = np.random.normal(0.9, 0.1, len(features))\n",
    "        elif label == \"PortScan\":\n",
    "            row = np.random.normal(0.8, 0.1, len(features))\n",
    "        elif label == \"Botnet\":\n",
    "            row = np.random.normal(0.75, 0.1, len(features))\n",
    "        elif label == \"Infiltration\":\n",
    "            row = np.random.normal(0.7, 0.1, len(features))\n",
    "        elif \"Web Attack\" in label:\n",
    "            row = np.random.normal(0.6, 0.1, len(features))\n",
    "        elif label == \"Heartbleed\":\n",
    "            row = np.random.normal(0.95, 0.05, len(features))\n",
    "        elif label == \"ANOMALY\":\n",
    "            row = np.random.normal(1.0, 0.15, len(features))\n",
    "        else:\n",
    "            row = np.random.normal(0.5, 0.1, len(features))\n",
    "        data.append(row.tolist() + [label])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=features + [\"attack_type\"])\n",
    "df = shuffle(df).reset_index(drop=True)\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(f\"‚úÖ Realistic test dataset created ‚Üí {OUTPUT_PATH}\")\n",
    "print(f\"Features: {len(features)} | Classes: {len(classes)} | Rows: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b7d0e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model-aligned dataset generated successfully!\n",
      "üìÅ Saved to: D:\\RealTime_Alert_Analysis\\notebook\\model_aligned_network_dataset.csv\n",
      "üìä Attack distribution:\n",
      "attack_type\n",
      "BENIGN                      1315\n",
      "Botnet                       526\n",
      "PortScan                     526\n",
      "DDoS                         526\n",
      "Normal                       526\n",
      "ANOMALY                      500\n",
      "Web Attack ‚Äì Brute Force     421\n",
      "Web Attack ‚Äì XSS             421\n",
      "Heartbleed                   368\n",
      "Infiltration                 368\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ======================================\n",
    "# CONFIGURATION\n",
    "# ======================================\n",
    "OUTPUT_PATH = r\"D:\\RealTime_Alert_Analysis\\notebook\\model_aligned_network_dataset.csv\"\n",
    "NUM_SAMPLES = 5000\n",
    "NUM_FEATURES = 78\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# ======================================\n",
    "# ATTACK LABELS & RISK LEVELS\n",
    "# ======================================\n",
    "attack_types = {\n",
    "    \"BENIGN\": \"Low\",\n",
    "    \"Normal\": \"Low\",\n",
    "    \"Botnet\": \"Medium\",\n",
    "    \"Web Attack ‚Äì XSS\": \"Medium\",\n",
    "    \"Web Attack ‚Äì Brute Force\": \"High\",\n",
    "    \"Infiltration\": \"High\",\n",
    "    \"PortScan\": \"High\",\n",
    "    \"DDoS\": \"Critical\",\n",
    "    \"Heartbleed\": \"Critical\",\n",
    "    \"ANOMALY\": \"Critical\"\n",
    "}\n",
    "\n",
    "distribution = {\n",
    "    \"BENIGN\": 0.25,\n",
    "    \"Normal\": 0.10,\n",
    "    \"Botnet\": 0.10,\n",
    "    \"Web Attack ‚Äì XSS\": 0.08,\n",
    "    \"Web Attack ‚Äì Brute Force\": 0.08,\n",
    "    \"Infiltration\": 0.07,\n",
    "    \"PortScan\": 0.10,\n",
    "    \"DDoS\": 0.10,\n",
    "    \"Heartbleed\": 0.07\n",
    "}\n",
    "distribution = {k: v / sum(distribution.values()) for k, v in distribution.items()}\n",
    "\n",
    "# ======================================\n",
    "# CORRELATED FEATURE GENERATOR\n",
    "# ======================================\n",
    "def generate_correlated_data(base_mean, scale, correlation_strength, n_samples):\n",
    "    \"\"\"\n",
    "    Generate correlated network-like features with a shared signal and controlled noise.\n",
    "    \"\"\"\n",
    "    base_signal = np.random.normal(base_mean, scale, (n_samples, 1))\n",
    "    correlated = base_signal + np.random.normal(0, scale * (1 - correlation_strength), (n_samples, NUM_FEATURES))\n",
    "    correlated = np.clip(correlated, 0, None)  # Ensure no negative features\n",
    "    return correlated\n",
    "\n",
    "# ======================================\n",
    "# ATTACK BEHAVIOR PROFILES\n",
    "# ======================================\n",
    "def generate_attack_profile(label, n):\n",
    "    \"\"\"\n",
    "    Create distinct, correlated, semi-realistic features per attack type.\n",
    "    \"\"\"\n",
    "    if label == \"BENIGN\":\n",
    "        return generate_correlated_data(10, 2, 0.9, n)\n",
    "    elif label == \"Normal\":\n",
    "        return generate_correlated_data(20, 5, 0.85, n)\n",
    "    elif label == \"Botnet\":\n",
    "        return generate_correlated_data(50, 15, 0.8, n)\n",
    "    elif label == \"Web Attack ‚Äì XSS\":\n",
    "        return generate_correlated_data(80, 20, 0.75, n)\n",
    "    elif label == \"Web Attack ‚Äì Brute Force\":\n",
    "        return generate_correlated_data(150, 50, 0.7, n)\n",
    "    elif label == \"Infiltration\":\n",
    "        return generate_correlated_data(300, 100, 0.6, n)\n",
    "    elif label == \"PortScan\":\n",
    "        return generate_correlated_data(500, 150, 0.6, n)\n",
    "    elif label == \"DDoS\":\n",
    "        return generate_correlated_data(1000, 300, 0.5, n)\n",
    "    elif label == \"Heartbleed\":\n",
    "        return generate_correlated_data(2000, 600, 0.4, n)\n",
    "    else:  # ANOMALY\n",
    "        arr = np.random.normal(5000, 2000, (n, NUM_FEATURES))\n",
    "        arr += np.random.uniform(0, 10000, arr.shape)  # wild spikes\n",
    "        return np.abs(arr)\n",
    "\n",
    "# ======================================\n",
    "# BUILD DATASET\n",
    "# ======================================\n",
    "frames = []\n",
    "for attack, prob in distribution.items():\n",
    "    n = int(NUM_SAMPLES * prob)\n",
    "    data = generate_attack_profile(attack, n)\n",
    "    df_temp = pd.DataFrame(data, columns=[f\"feature_{i+1}\" for i in range(NUM_FEATURES)])\n",
    "    df_temp[\"attack_type\"] = attack\n",
    "    frames.append(df_temp)\n",
    "\n",
    "# Add anomalies\n",
    "num_anomalies = int(NUM_SAMPLES * 0.1)\n",
    "anom_data = generate_attack_profile(\"ANOMALY\", num_anomalies)\n",
    "df_anom = pd.DataFrame(anom_data, columns=[f\"feature_{i+1}\" for i in range(NUM_FEATURES)])\n",
    "df_anom[\"attack_type\"] = \"ANOMALY\"\n",
    "frames.append(df_anom)\n",
    "\n",
    "df = pd.concat(frames, ignore_index=True)\n",
    "df[\"Label\"] = df[\"attack_type\"]\n",
    "df[\"Risk_Level\"] = df[\"attack_type\"].map(attack_types)\n",
    "\n",
    "# Shuffle dataset\n",
    "df = df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "# Save dataset\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(\"‚úÖ Model-aligned dataset generated successfully!\")\n",
    "print(f\"üìÅ Saved to: {OUTPUT_PATH}\")\n",
    "print(\"üìä Attack distribution:\")\n",
    "print(df['attack_type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12401927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Realistic self-scaled dataset generated successfully!\n",
      "üìÅ Saved to: D:\\RealTime_Alert_Analysis\\notebook\\realistic_self_scaled_network_dataset.csv\n",
      "üìä Attack distribution:\n",
      "attack_type\n",
      "BENIGN                      1315\n",
      "Botnet                       526\n",
      "PortScan                     526\n",
      "DDoS                         526\n",
      "Normal                       526\n",
      "ANOMALY                      500\n",
      "Web Attack ‚Äì Brute Force     421\n",
      "Web Attack ‚Äì XSS             421\n",
      "Heartbleed                   368\n",
      "Infiltration                 368\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "OUTPUT_PATH = r\"D:\\RealTime_Alert_Analysis\\notebook\\realistic_self_scaled_network_dataset.csv\"\n",
    "NUM_SAMPLES = 5000\n",
    "NUM_FEATURES = 78\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "attack_types = {\n",
    "    \"BENIGN\": \"Low\",\n",
    "    \"Normal\": \"Low\",\n",
    "    \"Botnet\": \"Medium\",\n",
    "    \"Web Attack ‚Äì XSS\": \"Medium\",\n",
    "    \"Web Attack ‚Äì Brute Force\": \"High\",\n",
    "    \"Infiltration\": \"High\",\n",
    "    \"PortScan\": \"High\",\n",
    "    \"DDoS\": \"Critical\",\n",
    "    \"Heartbleed\": \"Critical\",\n",
    "    \"ANOMALY\": \"Critical\"\n",
    "}\n",
    "\n",
    "distribution = {\n",
    "    \"BENIGN\": 0.25,\n",
    "    \"Normal\": 0.10,\n",
    "    \"Botnet\": 0.10,\n",
    "    \"Web Attack ‚Äì XSS\": 0.08,\n",
    "    \"Web Attack ‚Äì Brute Force\": 0.08,\n",
    "    \"Infiltration\": 0.07,\n",
    "    \"PortScan\": 0.10,\n",
    "    \"DDoS\": 0.10,\n",
    "    \"Heartbleed\": 0.07\n",
    "}\n",
    "distribution = {k: v / sum(distribution.values()) for k, v in distribution.items()}\n",
    "\n",
    "def generate_attack_profile(label, n):\n",
    "    base_mean, base_std = {\n",
    "        \"BENIGN\": (10, 3),\n",
    "        \"Normal\": (20, 6),\n",
    "        \"Botnet\": (50, 10),\n",
    "        \"Web Attack ‚Äì XSS\": (80, 15),\n",
    "        \"Web Attack ‚Äì Brute Force\": (150, 40),\n",
    "        \"Infiltration\": (300, 70),\n",
    "        \"PortScan\": (500, 100),\n",
    "        \"DDoS\": (900, 200),\n",
    "        \"Heartbleed\": (2000, 400)\n",
    "    }.get(label, (1000, 250))\n",
    "    \n",
    "    data = np.random.normal(base_mean, base_std, (n, NUM_FEATURES))\n",
    "    data = np.clip(data, 0, None)\n",
    "    return data\n",
    "\n",
    "frames = []\n",
    "for attack, prob in distribution.items():\n",
    "    n = int(NUM_SAMPLES * prob)\n",
    "    df_part = pd.DataFrame(\n",
    "        generate_attack_profile(attack, n),\n",
    "        columns=[f\"feature_{i+1}\" for i in range(NUM_FEATURES)]\n",
    "    )\n",
    "    df_part[\"attack_type\"] = attack\n",
    "    frames.append(df_part)\n",
    "\n",
    "# Add anomalies (wild random noise)\n",
    "anom_n = int(NUM_SAMPLES * 0.1)\n",
    "anom_data = np.abs(np.random.normal(5000, 2000, (anom_n, NUM_FEATURES)))\n",
    "df_anom = pd.DataFrame(anom_data, columns=[f\"feature_{i+1}\" for i in range(NUM_FEATURES)])\n",
    "df_anom[\"attack_type\"] = \"ANOMALY\"\n",
    "frames.append(df_anom)\n",
    "\n",
    "df = pd.concat(frames, ignore_index=True)\n",
    "df[\"Risk_Level\"] = df[\"attack_type\"].map(attack_types)\n",
    "df[\"Label\"] = df[\"attack_type\"]\n",
    "\n",
    "# ‚úÖ Scale numerically for model input consistency\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df.iloc[:, :-3]), columns=df.columns[:-3])\n",
    "df_scaled[\"attack_type\"] = df[\"attack_type\"]\n",
    "df_scaled[\"Risk_Level\"] = df[\"Risk_Level\"]\n",
    "df_scaled[\"Label\"] = df[\"Label\"]\n",
    "\n",
    "# Shuffle dataset\n",
    "df_scaled = df_scaled.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "df_scaled.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(\"‚úÖ Realistic self-scaled dataset generated successfully!\")\n",
    "print(f\"üìÅ Saved to: {OUTPUT_PATH}\")\n",
    "print(\"üìä Attack distribution:\")\n",
    "print(df_scaled['attack_type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed77ded1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Scaler loaded successfully!\n",
      "Mean shape: (78,)\n",
      "Var shape: (78,)\n",
      "\n",
      "üìä Example feature statistics (first 10):\n",
      "Feature 1: mean=0.0000, std=0.0000\n",
      "Feature 2: mean=0.0000, std=0.0000\n",
      "Feature 3: mean=0.0000, std=0.0000\n",
      "Feature 4: mean=0.0000, std=0.0000\n",
      "Feature 5: mean=0.0000, std=0.0000\n",
      "Feature 6: mean=0.0000, std=0.0000\n",
      "Feature 7: mean=0.0000, std=0.0000\n",
      "Feature 8: mean=0.0000, std=0.0000\n",
      "Feature 9: mean=0.0000, std=0.0000\n",
      "Feature 10: mean=0.0000, std=0.0000\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "SCALER_PATH = r\"D:\\RealTime_Alert_Analysis\\report\\training_report\\standard_scaler.joblib\"\n",
    "\n",
    "try:\n",
    "    scaler = joblib.load(SCALER_PATH)\n",
    "    print(\"‚úÖ Scaler loaded successfully!\")\n",
    "    print(f\"Mean shape: {scaler.mean_.shape}\")\n",
    "    print(f\"Var shape: {scaler.var_.shape}\\n\")\n",
    "    print(\"üìä Example feature statistics (first 10):\")\n",
    "    for i in range(min(10, len(scaler.mean_))):\n",
    "        print(f\"Feature {i+1}: mean={scaler.mean_[i]:.4f}, std={np.sqrt(scaler.var_[i]):.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading scaler: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c925d187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model components:\n",
      "dict_keys(['classifier', 'isolation_forest', 'label_encoder', 'class_names', 'feature_names'])\n",
      "\n",
      "üéØ LabelEncoder classes:\n",
      "[0.]\n",
      "\n",
      "üå≤ RandomForest info:\n",
      "n_classes_: 1\n",
      "n_features_in_: 78\n",
      "n_estimators: 150\n",
      "Trained classes: [0]\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "MODEL_PATH = r\"D:\\RealTime_Alert_Analysis\\report\\training_report\\trained_model_balanced_final_fixed.joblib\"\n",
    "model = joblib.load(MODEL_PATH)\n",
    "clf = model.get(\"classifier\", None)\n",
    "le = model.get(\"label_encoder\", None)\n",
    "\n",
    "print(\"‚úÖ Model components:\")\n",
    "print(model.keys())\n",
    "\n",
    "if le:\n",
    "    print(\"\\nüéØ LabelEncoder classes:\")\n",
    "    print(le.classes_)\n",
    "\n",
    "if clf:\n",
    "    print(f\"\\nüå≤ RandomForest info:\")\n",
    "    print(f\"n_classes_: {getattr(clf, 'n_classes_', 'N/A')}\")\n",
    "    print(f\"n_features_in_: {getattr(clf, 'n_features_in_', 'N/A')}\")\n",
    "    print(f\"n_estimators: {len(clf.estimators_)}\")\n",
    "    if hasattr(clf, \"classes_\"):\n",
    "        print(f\"Trained classes: {clf.classes_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6e5af45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Found 31 CSV files.\n",
      "\n",
      "‚úÖ Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv ‚Üí 79 columns\n",
      "‚úÖ Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv ‚Üí 79 columns\n",
      "‚úÖ Friday-WorkingHours-Morning.pcap_ISCX.csv ‚Üí 79 columns\n",
      "‚úÖ model_aligned_network_dataset.csv ‚Üí 81 columns\n",
      "‚úÖ Network_dataset_1.csv ‚Üí 46 columns\n",
      "‚úÖ Network_dataset_10.csv ‚Üí 46 columns\n",
      "‚úÖ Network_dataset_11.csv ‚Üí 46 columns\n",
      "‚úÖ Network_dataset_12.csv ‚Üí 46 columns\n",
      "‚úÖ Network_dataset_13.csv ‚Üí 46 columns\n",
      "‚úÖ Network_dataset_14.csv ‚Üí 46 columns\n",
      "‚úÖ Network_dataset_15.csv ‚Üí 46 columns\n",
      "‚úÖ Network_dataset_16.csv ‚Üí 46 columns\n",
      "‚úÖ Network_dataset_17.csv ‚Üí 46 columns\n",
      "‚úÖ Network_dataset_18.csv ‚Üí 46 columns\n",
      "‚úÖ Network_dataset_19.csv ‚Üí 46 columns\n",
      "‚úÖ Network_dataset_2.csv ‚Üí 46 columns\n",
      "‚úÖ Network_dataset_20.csv ‚Üí 46 columns\n",
      "‚úÖ Network_dataset_21.csv ‚Üí 46 columns\n",
      "‚úÖ Network_dataset_22.csv ‚Üí 46 columns\n",
      "‚úÖ Network_dataset_23.csv ‚Üí 46 columns\n",
      "‚úÖ Network_dataset_3.csv ‚Üí 46 columns\n",
      "‚úÖ Network_dataset_4.csv ‚Üí 46 columns\n",
      "‚úÖ Network_dataset_5.csv ‚Üí 46 columns\n",
      "‚úÖ Network_dataset_6.csv ‚Üí 47 columns\n",
      "‚úÖ Network_dataset_7.csv ‚Üí 46 columns\n",
      "‚úÖ Network_dataset_8.csv ‚Üí 46 columns\n",
      "‚úÖ Network_dataset_9.csv ‚Üí 46 columns\n",
      "‚úÖ realistic_balanced_attacks.csv ‚Üí 79 columns\n",
      "‚úÖ synthetic_attacks_test.csv ‚Üí 79 columns\n",
      "‚úÖ Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv ‚Üí 79 columns\n",
      "‚úÖ Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv ‚Üí 79 columns\n",
      "\n",
      "üìÑ Column report saved to: D:\\RealTime_Alert_Analysis\\report\\training_report\\column_report.txt\n",
      "‚úÖ Please send me the text content of that file (or screenshot).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "# ======================================\n",
    "# CONFIG\n",
    "# ======================================\n",
    "DATA_DIR = r\"D:\\RealTime_Alert_Analysis\\dataset\"   # üëà Change if needed\n",
    "OUTPUT_FILE = r\"D:\\RealTime_Alert_Analysis\\report\\training_report\\column_report.txt\"\n",
    "\n",
    "# ======================================\n",
    "# SCAN ALL CSV FILES\n",
    "# ======================================\n",
    "csv_files = glob(os.path.join(DATA_DIR, \"*.csv\"))\n",
    "if not csv_files:\n",
    "    print(f\"‚ùå No CSV files found in {DATA_DIR}\")\n",
    "    exit()\n",
    "\n",
    "report_lines = []\n",
    "print(f\"üì¶ Found {len(csv_files)} CSV files.\\n\")\n",
    "\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file, nrows=5)  # read first 5 rows for speed\n",
    "        cols = list(df.columns)\n",
    "        report_lines.append(f\"\\nüìÅ {os.path.basename(file)} ({len(cols)} columns)\\n\" + \"\\n\".join([f\"  - {c}\" for c in cols]))\n",
    "        print(f\"‚úÖ {os.path.basename(file)} ‚Üí {len(cols)} columns\")\n",
    "    except Exception as e:\n",
    "        report_lines.append(f\"\\n‚ö†Ô∏è {os.path.basename(file)}: Error reading file ‚Üí {e}\")\n",
    "        print(f\"‚ö†Ô∏è Skipped {file}: {e}\")\n",
    "\n",
    "# ======================================\n",
    "# FIND COMMON AND UNIQUE COLUMNS\n",
    "# ======================================\n",
    "all_col_sets = []\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file, nrows=1)\n",
    "        all_col_sets.append(set(df.columns))\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "if all_col_sets:\n",
    "    common_cols = set.intersection(*all_col_sets)\n",
    "    all_cols = set.union(*all_col_sets)\n",
    "\n",
    "    report_lines.append(\"\\n\\n===============================\")\n",
    "    report_lines.append(f\"‚úÖ COMMON COLUMNS ({len(common_cols)}):\\n\" + \"\\n\".join(sorted(common_cols)))\n",
    "    report_lines.append(\"\\n\\n===============================\")\n",
    "    report_lines.append(f\"üß© TOTAL UNIQUE COLUMNS ({len(all_cols)}):\\n\" + \"\\n\".join(sorted(all_cols)))\n",
    "\n",
    "# ======================================\n",
    "# SAVE REPORT\n",
    "# ======================================\n",
    "os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(report_lines))\n",
    "\n",
    "print(f\"\\nüìÑ Column report saved to: {OUTPUT_FILE}\")\n",
    "print(\"‚úÖ Please send me the text content of that file (or screenshot).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25819c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loading model: D:\\RealTime_Alert_Analysis\\report\\training_report\\universal_trained_model_fixed_v7.joblib\n",
      "‚úÖ Loading scaler: D:\\RealTime_Alert_Analysis\\report\\training_report\\universal_standard_scaler_fixed_v7.joblib\n",
      "üß© Model features: 197\n",
      "üì¶ Loading dataset: D:\\RealTime_Alert_Analysis\\dataset\\Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
      "‚úÖ CSV loaded: 225745 rows, 79 columns\n",
      "\n",
      "üßπ Cleaning dataset before prediction...\n",
      "üîç Missing columns filled: 0\n",
      "üîç Extra columns ignored: 0\n",
      "üìä Dataset ready: (225745, 197)\n",
      "üìè Scaling complete ‚Äî shape: (225745, 197)\n",
      "   Scaled mean: nan std: nan\n",
      "\n",
      "ü§ñ Running classification and anomaly detection...\n",
      "‚ö†Ô∏è IsolationForest issue: Input X contains NaN.\n",
      "IsolationForest does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "üìà Prediction Summary:\n",
      "BENIGN    225745\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚ö†Ô∏è All records predicted as a single label ‚Äî likely feature misalignment or zero variance after scaling.\n",
      "   ‚Üí Check dataset distribution and missing columns above.\n",
      "\n",
      "üíæ Results exported: D:\\RealTime_Alert_Analysis\\report\\predicted_output_v7.csv\n",
      "\n",
      "üéâ Prediction completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import time\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "# ============================================================\n",
    "# üîß CONFIGURATION\n",
    "# ============================================================\n",
    "MODEL_PATH = r\"D:\\RealTime_Alert_Analysis\\report\\training_report\\universal_trained_model_fixed_v7.joblib\"\n",
    "SCALER_PATH = r\"D:\\RealTime_Alert_Analysis\\report\\training_report\\universal_standard_scaler_fixed_v7.joblib\"\n",
    "CSV_PATH = r\"D:\\RealTime_Alert_Analysis\\dataset\\Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\"   # üëà Change this to your test CSV\n",
    "\n",
    "# ============================================================\n",
    "# üöÄ LOAD MODEL + SCALER\n",
    "# ============================================================\n",
    "print(f\"‚úÖ Loading model: {MODEL_PATH}\")\n",
    "bundle = joblib.load(MODEL_PATH)\n",
    "\n",
    "print(f\"‚úÖ Loading scaler: {SCALER_PATH}\")\n",
    "scaler = joblib.load(SCALER_PATH)\n",
    "\n",
    "clf = bundle[\"classifier\"]\n",
    "iso = bundle[\"isolation_forest\"]\n",
    "le = bundle[\"label_encoder\"]\n",
    "model_features = bundle[\"feature_names\"]\n",
    "\n",
    "print(f\"üß© Model features: {len(model_features)}\")\n",
    "\n",
    "# ============================================================\n",
    "# üìÇ LOAD DATA\n",
    "# ============================================================\n",
    "print(f\"üì¶ Loading dataset: {CSV_PATH}\")\n",
    "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
    "print(f\"‚úÖ CSV loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "# ============================================================\n",
    "# üßπ CLEAN & ALIGN\n",
    "# ============================================================\n",
    "print(\"\\nüßπ Cleaning dataset before prediction...\")\n",
    "\n",
    "# Keep only numeric columns\n",
    "df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Align dataset columns to model‚Äôs feature order\n",
    "df = df.reindex(columns=model_features, fill_value=np.nan)\n",
    "missing_cols = set(model_features) - set(df.columns)\n",
    "extra_cols = set(df.columns) - set(model_features)\n",
    "\n",
    "print(f\"üîç Missing columns filled: {len(missing_cols)}\")\n",
    "print(f\"üîç Extra columns ignored: {len(extra_cols)}\")\n",
    "\n",
    "# Replace invalid numbers\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df = df.astype(np.float32)\n",
    "df.fillna(df.mean(numeric_only=True), inplace=True)\n",
    "\n",
    "print(f\"üìä Dataset ready: {df.shape}\")\n",
    "\n",
    "# ============================================================\n",
    "# ‚öôÔ∏è SCALING\n",
    "# ============================================================\n",
    "try:\n",
    "    X_scaled = scaler.transform(df)\n",
    "except NotFittedError:\n",
    "    print(\"‚ùå Scaler not fitted ‚Äî fitting on current dataset.\")\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(df)\n",
    "    joblib.dump(scaler, SCALER_PATH)\n",
    "    print(f\"üíæ New scaler saved at {SCALER_PATH}\")\n",
    "\n",
    "print(f\"üìè Scaling complete ‚Äî shape: {X_scaled.shape}\")\n",
    "print(\"   Scaled mean:\", np.mean(X_scaled).round(5), \n",
    "      \"std:\", np.std(X_scaled).round(5))\n",
    "\n",
    "# ============================================================\n",
    "# üß† PREDICTION\n",
    "# ============================================================\n",
    "print(\"\\nü§ñ Running classification and anomaly detection...\")\n",
    "\n",
    "y_pred = clf.predict(X_scaled)\n",
    "pred_labels = le.inverse_transform(y_pred)\n",
    "\n",
    "# Optional anomaly detection (IsolationForest)\n",
    "try:\n",
    "    anomaly_flags = iso.predict(X_scaled)\n",
    "    anomaly_rate = (anomaly_flags == -1).mean() * 100\n",
    "except Exception as e:\n",
    "    anomaly_rate = None\n",
    "    print(\"‚ö†Ô∏è IsolationForest issue:\", e)\n",
    "\n",
    "# ============================================================\n",
    "# üìä RESULTS\n",
    "# ============================================================\n",
    "pred_series = pd.Series(pred_labels)\n",
    "label_counts = pred_series.value_counts()\n",
    "\n",
    "print(\"\\nüìà Prediction Summary:\")\n",
    "print(label_counts)\n",
    "\n",
    "if len(label_counts) == 1:\n",
    "    print(\"\\n‚ö†Ô∏è All records predicted as a single label ‚Äî likely feature misalignment or zero variance after scaling.\")\n",
    "    print(\"   ‚Üí Check dataset distribution and missing columns above.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Multiple labels detected ‚Äî predictions seem valid.\")\n",
    "\n",
    "if anomaly_rate is not None:\n",
    "    print(f\"\\nüß© Anomaly Rate (IsolationForest): {anomaly_rate:.2f}%\")\n",
    "\n",
    "# ============================================================\n",
    "# üíæ EXPORT RESULTS\n",
    "# ============================================================\n",
    "output_path = r\"D:\\RealTime_Alert_Analysis\\report\\predicted_output_v7.csv\"\n",
    "out_df = df.copy()\n",
    "out_df[\"Predicted_Label\"] = pred_series\n",
    "if anomaly_rate is not None:\n",
    "    out_df[\"Anomaly_Flag\"] = anomaly_flags\n",
    "\n",
    "out_df.to_csv(output_path, index=False)\n",
    "print(f\"\\nüíæ Results exported: {output_path}\")\n",
    "\n",
    "print(\"\\nüéâ Prediction completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "759d4069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RealTime_Alert_Analysis\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/dhoogla/unswnb15?dataset_version_number=5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11.7M/11.7M [00:02<00:00, 5.05MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\gange\\.cache\\kagglehub\\datasets\\dhoogla\\unswnb15\\versions\\5\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"dhoogla/unswnb15\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91d449a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(175341, 36)\n",
      "Index(['dur', 'proto', 'service', 'state', 'spkts', 'dpkts', 'sbytes',\n",
      "       'dbytes', 'rate', 'sload', 'dload', 'sloss', 'dloss', 'sinpkt',\n",
      "       'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt',\n",
      "       'synack', 'ackdat', 'smean', 'dmean', 'trans_depth',\n",
      "       'response_body_len', 'ct_src_dport_ltm', 'ct_dst_sport_ltm',\n",
      "       'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'is_sm_ips_ports',\n",
      "       'attack_cat', 'label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_parquet(r\"D:\\RealTime_Alert_Analysis\\report\\training_report\\UNSW_NB15_training-set.parquet\")\n",
    "test = pd.read_parquet(r\"D:\\RealTime_Alert_Analysis\\report\\training_report\\UNSW_NB15_testing-set.parquet\")\n",
    "\n",
    "print(train.shape)\n",
    "print(train.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0076ac7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming download from 60817408 bytes (95440229 bytes left)...\n",
      "Resuming download from https://www.kaggle.com/api/v1/datasets/download/mrwellsdavid/unsw-nb15?dataset_version_number=1 (60817408/156257637) bytes left.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 149M/149M [00:08<00:00, 11.1MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\gange\\.cache\\kagglehub\\datasets\\mrwellsdavid\\unsw-nb15\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"mrwellsdavid/unsw-nb15\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "806b4cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ CICIDS Downloader + Cleaner\n",
      "\n",
      "==== CICIDS 2017 ====\n",
      "üì• Downloading: https://www.kaggle.com/api/v1/datasets/download/cicdataset/cicids2017\n"
     ]
    },
    {
     "ename": "ConnectTimeout",
     "evalue": "HTTPSConnectionPool(host='www.kaggle.com', port=443): Max retries exceeded with url: /api/v1/datasets/download/cicdataset/cicids2017 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000028A052AC5E0>, 'Connection to www.kaggle.com timed out. (connect timeout=None)'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32md:\\RealTime_Alert_Analysis\\venv\\lib\\site-packages\\urllib3\\connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\RealTime_Alert_Analysis\\venv\\lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[1;32md:\\RealTime_Alert_Analysis\\venv\\lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mConnectTimeoutError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32md:\\RealTime_Alert_Analysis\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32md:\\RealTime_Alert_Analysis\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:488\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    487\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32md:\\RealTime_Alert_Analysis\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\RealTime_Alert_Analysis\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32md:\\RealTime_Alert_Analysis\\venv\\lib\\site-packages\\urllib3\\connection.py:753\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    752\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[1;32m--> 753\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    754\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32md:\\RealTime_Alert_Analysis\\venv\\lib\\site-packages\\urllib3\\connection.py:207\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    210\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mConnectTimeoutError\u001b[0m: (<urllib3.connection.HTTPSConnection object at 0x0000028A052AC5E0>, 'Connection to www.kaggle.com timed out. (connect timeout=None)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32md:\\RealTime_Alert_Analysis\\venv\\lib\\site-packages\\requests\\adapters.py:644\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 644\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32md:\\RealTime_Alert_Analysis\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32md:\\RealTime_Alert_Analysis\\venv\\lib\\site-packages\\urllib3\\util\\retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='www.kaggle.com', port=443): Max retries exceeded with url: /api/v1/datasets/download/cicdataset/cicids2017 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000028A052AC5E0>, 'Connection to www.kaggle.com timed out. (connect timeout=None)'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectTimeout\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 105\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# DOWNLOAD DATASETS\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m==== CICIDS 2017 ====\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 105\u001b[0m \u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCICIDS2017_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUT_2017\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m extract_zip(OUT_2017, EXTRACT_2017)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m==== CICIDS 2018 ====\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 30\u001b[0m, in \u001b[0;36mdownload_file\u001b[1;34m(url, output_path)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müì• Downloading: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m headers \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m r:\n\u001b[0;32m     31\u001b[0m     r\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m     32\u001b[0m     total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(r\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent-length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32md:\\RealTime_Alert_Analysis\\venv\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\RealTime_Alert_Analysis\\venv\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\RealTime_Alert_Analysis\\venv\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32md:\\RealTime_Alert_Analysis\\venv\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32md:\\RealTime_Alert_Analysis\\venv\\lib\\site-packages\\requests\\adapters.py:665\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[0;32m    663\u001b[0m     \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n\u001b[0;32m    664\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, NewConnectionError):\n\u001b[1;32m--> 665\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    667\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ResponseError):\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RetryError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectTimeout\u001b[0m: HTTPSConnectionPool(host='www.kaggle.com', port=443): Max retries exceeded with url: /api/v1/datasets/download/cicdataset/cicids2017 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000028A052AC5E0>, 'Connection to www.kaggle.com timed out. (connect timeout=None)'))"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "\n",
    "# ======================================================\n",
    "# CONFIG\n",
    "# ======================================================\n",
    "DATA_DIR = r\"C:\\Users\\gange\\datasets\"\n",
    "CICIDS2017_URL = \"https://www.kaggle.com/api/v1/datasets/download/cicdataset/cicids2017\"\n",
    "CICIDS2018_URL = \"https://www.kaggle.com/api/v1/datasets/download/dhoogla/nfcsecicids2018v2\"\n",
    "\n",
    "OUT_2017 = os.path.join(DATA_DIR, \"CICIDS2017.zip\")\n",
    "OUT_2018 = os.path.join(DATA_DIR, \"CICIDS2018.zip\")\n",
    "\n",
    "EXTRACT_2017 = os.path.join(DATA_DIR, \"CICIDS2017\")\n",
    "EXTRACT_2018 = os.path.join(DATA_DIR, \"CICIDS2018\")\n",
    "\n",
    "OUTPUT_PARQUET = os.path.join(DATA_DIR, \"cicids_combined.parquet\")\n",
    "\n",
    "# ======================================================\n",
    "# Download with progress bar\n",
    "# ======================================================\n",
    "def download_file(url, output_path):\n",
    "    print(f\"üì• Downloading: {url}\")\n",
    "\n",
    "    headers = {}\n",
    "    with requests.get(url, stream=True, headers=headers) as r:\n",
    "        r.raise_for_status()\n",
    "        total = int(r.headers.get(\"content-length\", 0))\n",
    "\n",
    "        with open(output_path, \"wb\") as f, tqdm(\n",
    "            desc=\"Downloading\",\n",
    "            total=total,\n",
    "            unit=\"B\",\n",
    "            unit_scale=True,\n",
    "            unit_divisor=1024,\n",
    "        ) as bar:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                size = f.write(chunk)\n",
    "                bar.update(size)\n",
    "\n",
    "    print(f\"‚úÖ Saved: {output_path}\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Extract ZIP\n",
    "# ======================================================\n",
    "def extract_zip(src, dest):\n",
    "    print(f\"üì¶ Extracting {src} ...\")\n",
    "    with zipfile.ZipFile(src, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dest)\n",
    "    print(f\"‚úÖ Extracted to: {dest}\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Clean and unify columns\n",
    "# ======================================================\n",
    "def normalize_cols(df):\n",
    "    df.columns = (\n",
    "        df.columns.astype(str)\n",
    "        .str.lower()\n",
    "        .str.strip()\n",
    "        .str.replace('[^a-z0-9_]+', '_', regex=True)\n",
    "        .str.replace('__+', '_', regex=True)\n",
    "        .str.strip('_')\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Load CSVs inside folder recursively\n",
    "# ======================================================\n",
    "def load_all_csvs(folder):\n",
    "    dfs = []\n",
    "    for root, _, files in os.walk(folder):\n",
    "        for f in files:\n",
    "            if f.lower().endswith(\".csv\"):\n",
    "                fp = os.path.join(root, f)\n",
    "                print(f\"üìÑ Loading CSV: {fp}\")\n",
    "                try:\n",
    "                    df = pd.read_csv(fp)\n",
    "                    df = normalize_cols(df)\n",
    "                    dfs.append(df)\n",
    "                except Exception as e:\n",
    "                    print(\"‚ùå Error:\", e)\n",
    "    return dfs\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# MAIN SCRIPT\n",
    "# ======================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ CICIDS Downloader + Cleaner\")\n",
    "\n",
    "    # Create folder\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "    # -----------------------------\n",
    "    # DOWNLOAD DATASETS\n",
    "    # -----------------------------\n",
    "    print(\"\\n==== CICIDS 2017 ====\")\n",
    "    download_file(CICIDS2017_URL, OUT_2017)\n",
    "    extract_zip(OUT_2017, EXTRACT_2017)\n",
    "\n",
    "    print(\"\\n==== CICIDS 2018 ====\")\n",
    "    download_file(CICIDS2018_URL, OUT_2018)\n",
    "    extract_zip(OUT_2018, EXTRACT_2018)\n",
    "\n",
    "    # -----------------------------\n",
    "    # LOAD DATA\n",
    "    # -----------------------------\n",
    "    print(\"\\nüì• Loading all CICIDS 2017 datasets...\")\n",
    "    dfs_2017 = load_all_csvs(EXTRACT_2017)\n",
    "\n",
    "    print(\"\\nüì• Loading all CICIDS 2018 datasets...\")\n",
    "    dfs_2018 = load_all_csvs(EXTRACT_2018)\n",
    "\n",
    "    all_dfs = dfs_2017 + dfs_2018\n",
    "    print(f\"üìä Total datasets loaded: {len(all_dfs)}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # MERGE\n",
    "    # -----------------------------\n",
    "    print(\"üîÑ Merging all datasets...\")\n",
    "    mega_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "    print(\"üßπ Cleaning NaN...\")\n",
    "    mega_df.fillna(0, inplace=True)\n",
    "\n",
    "    # -----------------------------\n",
    "    # SAVE PARQUET\n",
    "    # -----------------------------\n",
    "    mega_df.to_parquet(OUTPUT_PARQUET)\n",
    "\n",
    "    print(\"\\nüéâ DONE!\")\n",
    "    print(f\"üíæ Combined dataset saved to:\\n{OUTPUT_PARQUET}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "478e909b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/ernie55ernie/improved-cicids2017-and-csecicids2018?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10.2G/10.2G [13:43<00:00, 13.3MB/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\gange\\.cache\\kagglehub\\datasets\\ernie55ernie\\improved-cicids2017-and-csecicids2018\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"ernie55ernie/improved-cicids2017-and-csecicids2018\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2764a697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå Total columns: 91\n",
      "\n",
      "üìå Column names:\n",
      "\n",
      "id\n",
      "Flow ID\n",
      "Src IP\n",
      "Src Port\n",
      "Dst IP\n",
      "Dst Port\n",
      "Protocol\n",
      "Timestamp\n",
      "Flow Duration\n",
      "Total Fwd Packet\n",
      "Total Bwd packets\n",
      "Total Length of Fwd Packet\n",
      "Total Length of Bwd Packet\n",
      "Fwd Packet Length Max\n",
      "Fwd Packet Length Min\n",
      "Fwd Packet Length Mean\n",
      "Fwd Packet Length Std\n",
      "Bwd Packet Length Max\n",
      "Bwd Packet Length Min\n",
      "Bwd Packet Length Mean\n",
      "Bwd Packet Length Std\n",
      "Flow Bytes/s\n",
      "Flow Packets/s\n",
      "Flow IAT Mean\n",
      "Flow IAT Std\n",
      "Flow IAT Max\n",
      "Flow IAT Min\n",
      "Fwd IAT Total\n",
      "Fwd IAT Mean\n",
      "Fwd IAT Std\n",
      "Fwd IAT Max\n",
      "Fwd IAT Min\n",
      "Bwd IAT Total\n",
      "Bwd IAT Mean\n",
      "Bwd IAT Std\n",
      "Bwd IAT Max\n",
      "Bwd IAT Min\n",
      "Fwd PSH Flags\n",
      "Bwd PSH Flags\n",
      "Fwd URG Flags\n",
      "Bwd URG Flags\n",
      "Fwd RST Flags\n",
      "Bwd RST Flags\n",
      "Fwd Header Length\n",
      "Bwd Header Length\n",
      "Fwd Packets/s\n",
      "Bwd Packets/s\n",
      "Packet Length Min\n",
      "Packet Length Max\n",
      "Packet Length Mean\n",
      "Packet Length Std\n",
      "Packet Length Variance\n",
      "FIN Flag Count\n",
      "SYN Flag Count\n",
      "RST Flag Count\n",
      "PSH Flag Count\n",
      "ACK Flag Count\n",
      "URG Flag Count\n",
      "CWR Flag Count\n",
      "ECE Flag Count\n",
      "Down/Up Ratio\n",
      "Average Packet Size\n",
      "Fwd Segment Size Avg\n",
      "Bwd Segment Size Avg\n",
      "Fwd Bytes/Bulk Avg\n",
      "Fwd Packet/Bulk Avg\n",
      "Fwd Bulk Rate Avg\n",
      "Bwd Bytes/Bulk Avg\n",
      "Bwd Packet/Bulk Avg\n",
      "Bwd Bulk Rate Avg\n",
      "Subflow Fwd Packets\n",
      "Subflow Fwd Bytes\n",
      "Subflow Bwd Packets\n",
      "Subflow Bwd Bytes\n",
      "FWD Init Win Bytes\n",
      "Bwd Init Win Bytes\n",
      "Fwd Act Data Pkts\n",
      "Fwd Seg Size Min\n",
      "Active Mean\n",
      "Active Std\n",
      "Active Max\n",
      "Active Min\n",
      "Idle Mean\n",
      "Idle Std\n",
      "Idle Max\n",
      "Idle Min\n",
      "ICMP Code\n",
      "ICMP Type\n",
      "Total TCP Flow Time\n",
      "Label\n",
      "Attempted Category\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = r\"D:\\RealTime_Alert_Analysis\\dataset\\CICIDS2017_improved\\tuesday.csv\"  # <- change this\n",
    "\n",
    "df = pd.read_csv(file_path, nrows=5)  # read only first 5 rows (fast)\n",
    "\n",
    "print(\"\\nüìå Total columns:\", len(df.columns))\n",
    "print(\"\\nüìå Column names:\\n\")\n",
    "for col in df.columns:\n",
    "    print(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cea23d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Column report saved to: D:\\RealTime_Alert_Analysis\\dataset\\column_report.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# =============================\n",
    "# CONFIG ‚Äî SET YOUR PATHS HERE\n",
    "# =============================\n",
    "DATASETS = [\n",
    "    r\"D:\\RealTime_Alert_Analysis\\dataset\\CICIDS2017_improved\",\n",
    "    r\"D:\\RealTime_Alert_Analysis\\dataset\\CSECICIDS2018_improved\",\n",
    "    r\"D:\\RealTime_Alert_Analysis\\dataset\\UNSW_NB15_training-set.csv\",\n",
    "    r\"D:\\RealTime_Alert_Analysis\\dataset\\UNSW_NB15_testing-set.csv\",\n",
    "]\n",
    "\n",
    "OUTPUT_FILE = r\"D:\\RealTime_Alert_Analysis\\dataset\\column_report.txt\"\n",
    "\n",
    "\n",
    "def get_columns_from_csv(file_path):\n",
    "    \"\"\"Read only header of CSV to extract column names safely.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, nrows=1)  # read only 1 row\n",
    "        return list(df.columns)\n",
    "    except Exception as e:\n",
    "        return [f\"ERROR reading file: {e}\"]\n",
    "\n",
    "\n",
    "def scan_datasets():\n",
    "    report_lines = []\n",
    "    report_lines.append(\"===== COLUMN REPORT =====\\n\\n\")\n",
    "\n",
    "    for path in DATASETS:\n",
    "\n",
    "        if os.path.isdir(path):\n",
    "            report_lines.append(f\"\\nüìÅ Folder: {path}\\n\")\n",
    "\n",
    "            for fname in os.listdir(path):\n",
    "                if fname.lower().endswith(\".csv\"):\n",
    "                    full_path = os.path.join(path, fname)\n",
    "                    report_lines.append(f\"\\n--- File: {fname} ---\")\n",
    "                    cols = get_columns_from_csv(full_path)\n",
    "                    for c in cols:\n",
    "                        report_lines.append(f\"{c}\")\n",
    "        else:\n",
    "            # Single CSV file\n",
    "            report_lines.append(f\"\\nüìÑ File: {path}\\n\")\n",
    "            cols = get_columns_from_csv(path)\n",
    "            for c in cols:\n",
    "                report_lines.append(f\"{c}\")\n",
    "\n",
    "    # Save output\n",
    "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(report_lines))\n",
    "\n",
    "    print(f\"\\n‚úÖ Column report saved to: {OUTPUT_FILE}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scan_datasets()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
